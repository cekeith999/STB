So it started by me remembering that i’ve used blenders script / internal console to add objects into my scene. Then, realizing that that script and console run off of python, as well as blenders addons, AND that ai runs with python code, i thought “why haven’t i seen a blender - ai tool or use case.” the only use case i saw was using external tools like gemini 2.0 to create files to run within blenders console, but that was 7 months ago (today is august 15 2025) and i havnet seen anything since. 

So , I started with gpt 5. I had it try to give me code to recreate an image of grass down to the smallest details like the grain of grass, but it failed. 

Then i asked it “ok is it possible for you to interact with my blender scene live, basically using your agent feature, to build this object? because the code you give may work in theory, but when i use it, it is nowhere near the input image i gave you”

It said no.

Then there was a bunch of back and forth discourse, about trouble shooting getting this grass picture made. Short story, unsuccessful.

Then i came back the next day (today august 15th 2025) and asked “ok now, im thinking of making a workflow or something similar, that lets me control blender using my voice or text, which interacts with blender using the python console or run text scripts. i would also want it to be able to see blender. something like google ai studio could work well as it can see my screen, but i dont know about interacting with it”

This bitch ass, mean ass, fuck ass gpt said : Short version: stop trying to “remote-control” Blender by mashing random scripts into the console. Build a tiny local RPC bridge inside Blender that:
listens on localhost


executes a small set of safe commands on Blender’s main thread


can return a live viewport screenshot (so your AI can “see”)


you drive it from voice/text (Google AI Studio, whatever) by sending JSON commands.


Below is a drop-in Blender add-on that does exactly that using only the Python standard library (XML-RPC + queues). No extra packages, no UI hacks, no deadlocks.
…

	IM LIKE WHO TF ARE YOU TALKING TO LIKE THAT (though i told it to be blunt and insensitive)

Then it gave me a long list of steps, talking about RPC bridges nd stuff.

I asked it to explain RPC bridges and the steps… it said:  What a “local RPC bridge” is
RPC = Remote Procedure Call. You expose a few functions (e.g., add_cube, run_op, screenshot) on a tiny server. Another program calls those functions like normal code—even though they’re running inside Blender.


Local = bound to 127.0.0.1 only. It’s on your machine, not the internet.


Bridge = glue between your voice/text agent and Blender. The agent doesn’t jam random scripts into the console; it calls clean, whitelisted functions.


Think of it as a little phone line into Blender: “add a cube”, “render me a viewport screenshot”, “run this allowed operator”.

What the add-on I gave you does
Starts a tiny XML-RPC server inside Blender on 127.0.0.1:8765.


Exposes a few safe endpoints:


add_cube(size, loc) — demo control


run_op(op_id, kwargs) — run whitelisted bpy.ops.* (you decide the list)


screenshot() — returns a base64 PNG of the active 3D view (so your AI can “see”)


list_objects() — returns object names (example of read-only queries)


Executes everything on Blender’s main thread (mandatory) using a queue + timer. No thread-unsafe bpy calls.
So now im like , “from the speech to text, to llm parser step, do i have to copy and paste anythign into ai studio?, and in this case actually, why would i use ai studio? could i use something else more effective at that step?”

Then gpt: You wouldn’t have to copy/paste anything into AI Studio unless you were using it purely as the “brain” for turning your transcript into commands.
The Speech-to-Text → LLM parser step can be handled by any LLM endpoint that:
Accepts your STT transcript as input.


Returns clean, structured JSON with the operation name + parameters you want to call in Blender.


AI Studio is just one possible choice for that LLM.
 Reasons you might choose it:
You want Google’s multimodal model (Gemini) that can also look at the screenshot from Blender and reason about what’s in frame.


It’s already integrated with your Google account and has low setup friction.


You don’t mind running it in the browser with your data on Google’s cloud.


Reasons you might skip it and use something else:
You want local/offline parsing (e.g., Ollama running a small instruct model).


You want OpenAI API or Anthropic Claude because they’re better at strict JSON or code generation.


You want direct integration into your script without needing to open a web UI.


You want full automation without manual copy/paste into a web page.



More effective options for the parser step:
OpenAI GPT-4o / GPT-4 Turbo — very good at tool-call JSON formatting.


Claude 3.5 Sonnet — great at following schema instructions.


Local models via Ollama or LM Studio (LLaMA 3, Mistral, Command R+) — good if you want offline privacy.


LangChain or LlamaIndex orchestrating your STT → LLM → RPC calls in one Python process.


So now im thinking, hmm… i want this to run offline, or locally, and i know i can use an api key to make it happen so i dont have to use gemini online…so how would that look?

I then told gpt that i wanted to use my api key, and it gave me this workflow: 
Mic → Whisper STT → GPT-4o → Blender RPC

From there, i got to work. I first worked on the Mic using OpenAIs Whisper - i believe their speech to text software. But it talked about RPC again so i asked what is it. I found out that an RPC is basically a local server blender can host itself. Pretty cool.

So, to work with the whisper files, i downloaded the tiny whisper file to make sure this ran, and could run fast.

Ran the code to download from git or huggingface, i dont remember. Then, had it explain the steps to me again to make sure i understood what i was doing. Building the mic, building the pathway to get the transcription, then send it to OpenAI with my api, then back to blender with the RPC.

So after downloading all the files, whispers, tiny, and running some tests, . i asked “what would that mic capture look like, ideally. i would have it running and i just talk and it hears me, clips the audio, transcribes, and sends to 4o. but how would it go from stt whisper, to 4o?”it said: Exactly — ideally it’s a hot mic loop

I asked “ok where would that initial mic capture loop run though, and how does it run the whisper afterwards” it said: The mic capture loop wouldn’t run inside Blender — it would run as a separate Python process on your machine, because:
Blender’s embedded Python is not great at running real-time audio/VAD loops (it’s blocking and tied to the UI thread).


If we keep it separate, you can start/stop voice control without touching Blender’s UI.


It’s easier to talk to external programs like whisper.cpp from a normal Python runtime.


Now,  it said to run it through my terminal. Its the python file that runs, which runs everything else. So, when i run it through terminal, it activates the mic, transcribe, api, then blender.

After getting whisper (the speech to text) working, the next step was to take the transcript and send it to 4o with an api call. Using another voice to blender python file.

After installing the python dependencies and creating the files, i had a working voice to blender workflow

Then after a lot of troubleshooting and debugging. I ran the files in my terminal. Spoke. And a cube was made.




Next came improvements. Because it worked, i wanted it to work well. Making it so that i can add anything, which is just a few lines of code being changed. And, so that i can zip it, and run it on other computers.

To zip it, i have to add python files (exe, and .py) along with a lot of other files like init___.py, readmes, and everything else made locally, to that one file so that the entire process is portable. I am almost done making it portable.




Aug 16, 2025

Today i was really just trying to integrate the meshy api into the pipeline, so you can generate abstract models. It works for the most part, but i am running into trouble. The models generated are low quality. I want them to be meshy v5 quality. So i joined the discord to ask about my problem. Will be back soon.


Aug 17, 2025

So today meshy is still on hold. But i added import export capabilities. I havent tested export capabilities yet though. I will do so tomorrow before starting. Then i will work (probably for a while i suspect, because this is getting into agentic modes i believe) but work on adding edit mode functionality.

August 18, 2025.

Apparently, the tool didnt have the foundation to be easily built on in the future , with other models, workflows, etc. so i am going through this 10 step process to make it scalable easily:
1) Config & Secrets (no more hard-coded anything)
One config file the addon and voice script can both read. Use pure-stdlib JSON/TOML so it works inside Blender.


Per-provider blocks so you can drop in more API keys and switches without editing code.


Profiles: "dev" | "prod" | "mock" to flip behavior fast.
2) Providers: a tiny plugin interface
Abstract every external model behind the same interface. Adding Meshy, “Another3D,” OpenAI, Stability, local server… becomes a one-file drop.
3) Commands: a single schema, many verbs
Keep natural language on the voice side; on the Blender side normalize into a small set of safe, typed commands. That makes expansion easy and safety enforceable.
4) Pipeline: one orchestrator to rule them all
Make a small state machine so every request follows the same life cycle: PARSE → VALIDATE → DISPATCH(provider) → POLL → FETCH → POSTPROCESS → IMPORT.
5) Process boundaries that won’t fight you
Addon (inside Blender): thin UI, command executor, importers, safety guard.


Worker (outside Blender): voice I/O, LLM parsing, provider calls, long polls.


Talk over XML-RPC or HTTP. You already have XML-RPC; that’s fine. If you ever swap to HTTP, use stdlib http.server to avoid packaging headaches.
6) Diagnostics you’ll actually use
Unified logger (stdlib logging) that prefixes each line with PID, component, and a short request id. Log to file and console.


Progress callbacks: feed provider progress back to the UI panel. You already wanted this — wire on_progress to a simple shared file or socket that the addon reads on a timer to update a progress bar.


Human-readable failures: wrap provider errors so Blender’s console shows: Provider=meshy • op=text2mesh • 404 NoMatchingRoute • tip=check endpoint /v1/text-to-3d.


7) Tests & mocks that unblock you when APIs flake
Tiny unit tests for: command parsing, safety gate, provider adapters (mock HTTP with canned JSON), and importer paths.


A mock provider that pretends to run and spits out a known .fbx from tests/fixtures/. Flip profile="mock" to demo end-to-end without internet.



August 24th, 2025

Been working on it for the past few days. adding the foundational architecture has been a task to say the least. but right now, i am working on transferring to Cursor to do the coding work, instead of going back and forth between GPT and the code.

Still have to do:

8) Safety rail that scales with capability
Your earlier “only add cube” restriction becomes a policy not a hack.
9) Version the public contract (future-you will thank present-you)
Add "api_version": "0.2" to every message between processes.


Reject mismatches loudly. This keeps refactors from silently breaking voice control.


10) Adding a new model tomorrow = 1 file + 3 lines
Create stb_core/providers/another3d.py implementing the 4 methods.


Register it in registry.py.


Add a block in config.json and point "active_provider": "another3d" (or select per-task).

Its october 31st, 2025. And i havent touched this in a minute, i have a lot going on right now, but i will be putting time into this every friday. 
Where i remember, its a speech to blender addon, that allows for a few functions to use your local computers microphone, openais speech to text software to transcribe, then send that transcription (i believe wrapped in a standard llm script) to gpt formatted in json with an api, then it returns a bpy script with your request, then runs the script automatically within blender. The bridge between blender and everything else, is an RPC bridge. RPC = Remote Procedure Call. You expose a few functions (e.g., add_cube, run_op, screenshot) on a tiny server. Another program calls those functions like normal code—even though they’re running inside Blender.

Where my thoughts are right now, is that i want a method of integrating an ai model to blender / 3d softwares, it doesn't have to be a completely new model but it should be able to interact with your scene in any way possible. Blender is a good benchmark, as i know how to use it, and it covers a wide range of 3d capabilities and workflows. I imagine, for newcomers, they can jump into blender and have the "assistant" next to them, possibly first thing to do is ask them what they'd like to do. If they have previous experience within 3d and or blender, then they can choose a hands off approach to assisting, but if they are a new newcomer with no experience, then help guide along the way to achieving a goal. It would be able to have these functionalities:
Agent: complete projects start to finish for you. Build scene, models, sculpt, animate, and have capabilities to even build out complete geo node systems, rigging and character animations, as well as realistic shaders.
Teacher: Works with you to accomplish the project, rather than doing it solely for you. Kind of like a middle ground between a standard question and answer llm, and an agentic workflow. Especially if you are working to learn.
Simple Assistant: Lowest level of this, as it would essentially be a call and response. This includes: STB (ask for something, get it returned), one shot prompting (prompt for a thing.
Partnership: this has a model being able to work in the background with you to build a scene, hit you up when it has a question. This could utilize NVIDIA Omniverse as it allows for multiple people to work together on one file at the same time. Like a google doc. But, do you need Omniverse to do this? But even with this, you could then have multiple agents working together, holy fuck you could have a fucking team of agents working on your scenes, building an entire production just from a good fucking prompt. But again do you need NVIDIA Omniverse to do this? This reminds me of Grok 4 Super, as you can have 1 high level agent, guiding 4 lower level agents all for one task (creating a movie). This immediately feels like a combination of agent and the ability to talk live to an agent. 
Ok so this is where I am, and now is to get back on track, flesh out the high level idea. And focus on a low level objective. 


Ok so I think after consulting the council (gpt, gemini and claude), the MVP for this is Speech To Blender. I think I should get it working again, smoothly, video it. Then figure out next steps.



When you get in it, stay consistent, the motivation will come.




Thoughts: so I think a problem I may run into, is getting correct context for the task at hand. It may have to keep track of all the changes, and predict what the scene is going to look like, to help it suit the next request better. So almost keeping a log of all the changes as they're made, then assessing the scene and etc, and developing context of what's being made. And possible next steps


November 7, 2025 - 2:15pm

Today we focused on getting the voice command functionality working smoothly and improving performance. Major changes:

1. Fixed voice command parsing issues:
   - Added word-to-number conversion (e.g., "three" → 3) to handle spoken numbers
   - Improved natural language matching for delete commands ("delete all" now works, not just "delete selected")
   - Added multi-command parsing - commands can now be split on "and", "then", commas, etc. (e.g., "select all and delete" executes both commands)
   - Fixed plural matching for object types ("spheres" now matches "sphere")
   - Fixed deselect command priority (checks deselect before select to avoid false matches)

2. Enabled GPT fallback for natural language understanding:
   - Set ENABLE_GPT_FALLBACK = True
   - GPT-4o now handles commands when local rules don't match
   - Works with improved local rules for better coverage

3. Added OpenAI API key management to Blender preferences:
   - Created split fields (Part 1 and Part 2) to work around Blender's ~127-character StringProperty limit
   - Full 164-character project keys (sk-proj-*) can now be stored by splitting into two fields
   - Automatic combination of parts when retrieving the key
   - UI shows combined key length and validation feedback
   - Added RPC method to retrieve key from preferences

4. Bundled openai package and added auto-install:
   - Created requirements.txt documenting dependencies
   - Added auto-install function that checks and installs openai if missing
   - Package is pre-installed in stb_runtime for distribution
   - Created BUNDLING.md documentation

5. Performance optimizations:
   - Removed unnecessary key validation test (saved ~500ms-2s per GPT call)
   - Added API key caching to avoid repeated RPC calls (saved ~20-100ms per call)
   - Reduced sleep delays: 0.2s → 0.05s, 0.05s → 0.01s, 0.3s → 0.1s (saved ~150-250ms per command)
   - Disabled verbose debug mode (VERBOSE_DEBUG = False) for cleaner, faster output
   - Total performance improvement: ~670ms-2.35s faster per command

The voice workflow is now much more responsive and handles natural language commands better. The split API key fields solve the Blender storage limitation, and caching eliminates redundant RPC calls.

November 7, 2025 - 3:45pm

Today we implemented Phase 1 of "Super Mode" - an advanced AI-driven modeling mode for context-aware, intelligent object manipulation. This is the foundation for future phases that will enable complex edit mode operations, mesh analysis, and AI-guided modeling workflows.

Phase 1 Implementation (Super Mode Toggle & Basic Infrastructure):
1. Added Super Mode UI and preferences:
   - Created new "Voice Mode" panel (STB_PT_VoiceMode) in Blender's add-on UI
   - Added toggle button for "Super Mode" with visual icon feedback
   - Added "Target Object" preference field to specify which object Super Mode should focus on
   - Super Mode state stored in WindowManager property (stb_super_mode) for runtime control

2. RPC integration for Super Mode:
   - Added get_super_mode_state() RPC method to query Super Mode status from voice script
   - Returns enabled status and target_object name
   - Voice script can now check Super Mode state and route commands accordingly

3. Voice script integration:
   - Added SUPER_MODE_ENABLED and SUPER_MODE_TARGET globals in voice_to_blender.py
   - Voice script queries Super Mode state from RPC at startup and during command processing
   - Prepared infrastructure for Phase 2+ where Super Mode will route to enhanced GPT prompts with mesh context

4. Fixed console window and script stability:
   - Removed stdout/stderr redirection from subprocess.Popen to allow console window to appear
   - Created batch file wrapper (voice_to_blender.bat) for Windows to keep console open on errors
   - Added comprehensive startup checks for numpy, sounddevice, and RPC connection
   - Added 10-second pause on fatal errors to prevent silent crashes
   - Fixed syntax errors related to global variable declarations

5. Undo grouping improvements:
   - Each voice command (even multi-step ones) now creates a single undo point
   - Added start_voice_command() RPC method to mark the beginning of a voice command
   - Voice script calls this at the start of each command processing loop
   - Blender's Ctrl+Z now undoes one voice command at a time instead of all actions

Super Mode is planned as a 5-phase implementation:
- Phase 1: ✅ Toggle & Basic Infrastructure (COMPLETE)
- Phase 2: Context Functions (get_modeling_context, analyze_current_mesh)
- Phase 3: Enhanced GPT Prompts with mesh analysis and reference knowledge
- Phase 4: Complex Operation Executor - Support ANY Blender operation (modifiers, transforms, topology, etc.) with GPT choosing the best approach
- Phase 5: Edit Mode Commands (loop cuts, subdivide, select by region)

The foundation is now in place for context-aware AI modeling that can understand mesh geometry, vertex positions, and choose the best Blender operations to achieve desired effects.

---

## Super Mode: Architecture & Design Philosophy

### The Core Problem

Traditional voice-to-Blender workflows are limited to simple, pre-defined commands ("add cube", "delete selected"). They can't handle complex, context-dependent modeling tasks like:
- "Make this look more like an Amazon Echo Dot" (requires understanding mesh topology, vertex patterns, and reference knowledge)
- "Add edge loops around the middle section" (requires understanding current geometry and choosing the right Blender operation)
- "Smooth out the corners but keep the overall shape" (requires mesh analysis and multi-step operations)

Super Mode addresses this by combining:
1. **Real-time mesh context** - Understanding the current state of the 3D scene
2. **Reference knowledge** - Leveraging training data from models that understand common 3D object structures
3. **Intelligent operation selection** - GPT choosing the best Blender operations to achieve desired effects
4. **Multi-step reasoning** - Breaking complex requests into sequential operations

### Models & Datasets

**Speech-to-Text: faster-whisper (small model)**
- **Why faster-whisper**: Faster inference than OpenAI's Whisper API, runs locally (privacy), no API costs for STT
- **Why "small" model**: Balance between accuracy (~95% for clear speech) and speed (~200-500ms transcription). "tiny" is too inaccurate, "base/medium" adds latency without significant gains for English
- **Language**: English-only for now (WHISPER_LANG="en") - can expand later
- **Dataset**: Trained on Common Voice, LibriSpeech, and other public speech datasets
- **Trade-off**: Local STT means no cloud dependency, but requires bundling the model (~500MB for small)

**Natural Language Understanding: GPT-4o**
- **Why GPT-4o**: 
  - Superior reasoning for complex, multi-step commands
  - Excellent at understanding context and intent
  - Strong JSON generation (critical for our structured command format)
  - Multimodal capabilities (future: can analyze viewport screenshots)
  - Fast response time (~1-3s) compared to GPT-4 Turbo
- **Why not GPT-3.5**: Less reliable for complex commands, weaker reasoning
- **Why not Claude/Sonnet**: GPT-4o has better tool-use JSON formatting, more consistent output
- **Why not local models (Ollama/Llama)**: Current local models lack the reasoning depth needed for complex 3D operations. Future: could use local model for simple commands, GPT-4o for Super Mode
- **Temperature**: 0 (deterministic) - we need consistent, reliable command generation
- **Dataset**: Trained on internet-scale data including code, technical documentation, and 3D modeling concepts

**Future: Mesh Analysis & Reference Knowledge**
- **Phase 2-3 will add**:
  - Mesh geometry extraction (vertex positions, edge topology, face data)
  - Shape analysis (detecting common patterns: cylinders, spheres, complex organic shapes)
  - Reference knowledge base (common vertex patterns for objects like Echo Dot, phone, etc.)
- **Why this matters**: GPT-4o has general knowledge about 3D objects, but doesn't know YOUR specific mesh. We need to feed it:
  1. Current mesh state (vertices, edges, faces)
  2. Target object context ("this should be an Echo Dot")
  3. Desired transformation ("make it smoother")
- **Training data considerations**: GPT-4o was trained on text/code, not raw mesh data. We bridge this by:
  - Converting mesh to structured text descriptions
  - Using GPT's understanding of geometric concepts
  - Providing operation examples in prompts

### Architecture Decisions

**Hybrid Local + Cloud Approach**
- **Local**: STT (faster-whisper), simple command parsing (regex rules), safety gate
- **Cloud**: Complex NLU (GPT-4o), future mesh analysis (could be local with specialized model)
- **Why**: Best of both worlds - fast, private for simple commands; powerful AI for complex tasks

**Two-Tier Command Processing**
1. **Normal Mode**: Fast path - local rules → GPT fallback (simple commands)
2. **Super Mode**: Enhanced path - local rules → GPT with mesh context → complex operations
- **Why separate modes**: Super Mode adds latency (mesh analysis, richer prompts). Users should opt-in for complex tasks.

**RPC Bridge Architecture**
- **Why XML-RPC**: Simple, stdlib-only, works inside Blender's Python environment
- **Why not HTTP/REST**: More complex, requires additional dependencies
- **Why not WebSocket**: Overkill for request/response pattern, adds complexity
- **Safety Gate**: Whitelist approach - only safe operators allowed. Prevents dangerous operations (file deletion, addon disabling, etc.)

**Undo Grouping Strategy**
- **Problem**: Multiple Blender operations from one voice command should be one undo step
- **Solution**: `bpy.ops.ed.undo_push(message="Voice Command")` after each command batch
- **Why**: User expects "Ctrl+Z" to undo their voice command, not individual operations

### Why Not Alternative Approaches?

**Why not pure local models?**
- Current local LLMs (Llama 3, Mistral) lack the reasoning depth for complex 3D operations
- Future: Could use local model for simple commands, GPT-4o for Super Mode (hybrid)

**Why not specialized 3D AI models?**
- No general-purpose 3D modeling AI exists yet (Shape-E, Point-E are generation-only)
- GPT-4o's general reasoning + our mesh context bridge is more flexible

**Why not rule-based system?**
- Rules work for simple commands ("add cube") but don't scale to complex requests
- GPT provides flexibility and natural language understanding

**Why not agentic workflow (autonomous agent)?**
- Too risky for Blender operations (could make unintended changes)
- User should maintain control - voice commands are explicit instructions, not autonomous agents

### Future Considerations

**Phase 2-5 will explore**:
- Mesh geometry extraction and analysis
- Reference knowledge integration (common object patterns)
- Edit mode operations (loop cuts, subdivide, vertex manipulation)
- Modifier-based operations (subdivision surface, bevel, etc.)
- Multi-object operations (coordinated changes across scene)

**Potential improvements**:
- Local model fallback for simple Super Mode commands (reduce API costs)
- Specialized mesh analysis model (trained on 3D geometry)
- Viewport screenshot analysis (GPT-4o vision for visual feedback)
- Learning from user corrections (fine-tuning on successful operations)

The goal is to make Super Mode feel like working with an expert 3D artist who understands both your intent and the technical details of Blender operations.
