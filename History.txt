okSo it started by me remembering that i‚Äôve used blenders script / internal console to add objects into my scene. Then, realizing that that script and console run off of python, as well as blenders addons, AND that ai runs with python code, i thought ‚Äúwhy haven‚Äôt i seen a blender - ai tool or use case.‚Äù the only use case i saw was using external tools like gemini 2.0 to create files to run within blenders console, but that was 7 months ago (today is august 15 2025) and i havnet seen anything since. 

So , I started with gpt 5. I had it try to give me code to recreate an image of grass down to the smallest details like the grain of grass, but it failed. 

Then i asked it ‚Äúok is it possible for you to interact with my blender scene live, basically using your agent feature, to build this object? because the code you give may work in theory, but when i use it, it is nowhere near the input image i gave you‚Äù

It said no.

Then there was a bunch of back and forth discourse, about trouble shooting getting this grass picture made. Short story, unsuccessful.

Then i came back the next day (today august 15th 2025) and asked ‚Äúok now, im thinking of making a workflow or something similar, that lets me control blender using my voice or text, which interacts with blender using the python console or run text scripts. i would also want it to be able to see blender. something like google ai studio could work well as it can see my screen, but i dont know about interacting with it‚Äù

This bitch ass, mean ass, fuck ass gpt said : Short version: stop trying to ‚Äúremote-control‚Äù Blender by mashing random scripts into the console. Build a tiny local RPC bridge inside Blender that:
listens on localhost


executes a small set of safe commands on Blender‚Äôs main thread


can return a live viewport screenshot (so your AI can ‚Äúsee‚Äù)


you drive it from voice/text (Google AI Studio, whatever) by sending JSON commands.


Below is a drop-in Blender add-on that does exactly that using only the Python standard library (XML-RPC + queues). No extra packages, no UI hacks, no deadlocks.
‚Ä¶

	IM LIKE WHO TF ARE YOU TALKING TO LIKE THAT (though i told it to be blunt and insensitive)

Then it gave me a long list of steps, talking about RPC bridges nd stuff.

I asked it to explain RPC bridges and the steps‚Ä¶ it said:  What a ‚Äúlocal RPC bridge‚Äù is
RPC = Remote Procedure Call. You expose a few functions (e.g., add_cube, run_op, screenshot) on a tiny server. Another program calls those functions like normal code‚Äîeven though they‚Äôre running inside Blender.


Local = bound to 127.0.0.1 only. It‚Äôs on your machine, not the internet.


Bridge = glue between your voice/text agent and Blender. The agent doesn‚Äôt jam random scripts into the console; it calls clean, whitelisted functions.


Think of it as a little phone line into Blender: ‚Äúadd a cube‚Äù, ‚Äúrender me a viewport screenshot‚Äù, ‚Äúrun this allowed operator‚Äù.

What the add-on I gave you does
Starts a tiny XML-RPC server inside Blender on 127.0.0.1:8765.


Exposes a few safe endpoints:


add_cube(size, loc) ‚Äî demo control


run_op(op_id, kwargs) ‚Äî run whitelisted bpy.ops.* (you decide the list)


screenshot() ‚Äî returns a base64 PNG of the active 3D view (so your AI can ‚Äúsee‚Äù)


list_objects() ‚Äî returns object names (example of read-only queries)


Executes everything on Blender‚Äôs main thread (mandatory) using a queue + timer. No thread-unsafe bpy calls.
So now im like , ‚Äúfrom the speech to text, to llm parser step, do i have to copy and paste anythign into ai studio?, and in this case actually, why would i use ai studio? could i use something else more effective at that step?‚Äù

Then gpt: You wouldn‚Äôt have to copy/paste anything into AI Studio unless you were using it purely as the ‚Äúbrain‚Äù for turning your transcript into commands.
The Speech-to-Text ‚Üí LLM parser step can be handled by any LLM endpoint that:
Accepts your STT transcript as input.


Returns clean, structured JSON with the operation name + parameters you want to call in Blender.


AI Studio is just one possible choice for that LLM.
 Reasons you might choose it:
You want Google‚Äôs multimodal model (Gemini) that can also look at the screenshot from Blender and reason about what‚Äôs in frame.


It‚Äôs already integrated with your Google account and has low setup friction.


You don‚Äôt mind running it in the browser with your data on Google‚Äôs cloud.


Reasons you might skip it and use something else:
You want local/offline parsing (e.g., Ollama running a small instruct model).


You want OpenAI API or Anthropic Claude because they‚Äôre better at strict JSON or code generation.


You want direct integration into your script without needing to open a web UI.


You want full automation without manual copy/paste into a web page.



More effective options for the parser step:
OpenAI GPT-4o / GPT-4 Turbo ‚Äî very good at tool-call JSON formatting.


Claude 3.5 Sonnet ‚Äî great at following schema instructions.


Local models via Ollama or LM Studio (LLaMA 3, Mistral, Command R+) ‚Äî good if you want offline privacy.


LangChain or LlamaIndex orchestrating your STT ‚Üí LLM ‚Üí RPC calls in one Python process.


So now im thinking, hmm‚Ä¶ i want this to run offline, or locally, and i know i can use an api key to make it happen so i dont have to use gemini online‚Ä¶so how would that look?

I then told gpt that i wanted to use my api key, and it gave me this workflow: 
Mic ‚Üí Whisper STT ‚Üí GPT-4o ‚Üí Blender RPC

From there, i got to work. I first worked on the Mic using OpenAIs Whisper - i believe their speech to text software. But it talked about RPC again so i asked what is it. I found out that an RPC is basically a local server blender can host itself. Pretty cool.

So, to work with the whisper files, i downloaded the tiny whisper file to make sure this ran, and could run fast.

Ran the code to download from git or huggingface, i dont remember. Then, had it explain the steps to me again to make sure i understood what i was doing. Building the mic, building the pathway to get the transcription, then send it to OpenAI with my api, then back to blender with the RPC.

So after downloading all the files, whispers, tiny, and running some tests, . i asked ‚Äúwhat would that mic capture look like, ideally. i would have it running and i just talk and it hears me, clips the audio, transcribes, and sends to 4o. but how would it go from stt whisper, to 4o?‚Äùit said: Exactly ‚Äî ideally it‚Äôs a hot mic loop

I asked ‚Äúok where would that initial mic capture loop run though, and how does it run the whisper afterwards‚Äù it said: The mic capture loop wouldn‚Äôt run inside Blender ‚Äî it would run as a separate Python process on your machine, because:
Blender‚Äôs embedded Python is not great at running real-time audio/VAD loops (it‚Äôs blocking and tied to the UI thread).


If we keep it separate, you can start/stop voice control without touching Blender‚Äôs UI.


It‚Äôs easier to talk to external programs like whisper.cpp from a normal Python runtime.


Now,  it said to run it through my terminal. Its the python file that runs, which runs everything else. So, when i run it through terminal, it activates the mic, transcribe, api, then blender.

After getting whisper (the speech to text) working, the next step was to take the transcript and send it to 4o with an api call. Using another voice to blender python file.

After installing the python dependencies and creating the files, i had a working voice to blender workflow

Then after a lot of troubleshooting and debugging. I ran the files in my terminal. Spoke. And a cube was made.




Next came improvements. Because it worked, i wanted it to work well. Making it so that i can add anything, which is just a few lines of code being changed. And, so that i can zip it, and run it on other computers.

To zip it, i have to add python files (exe, and .py) along with a lot of other files like init___.py, readmes, and everything else made locally, to that one file so that the entire process is portable. I am almost done making it portable.




Aug 16, 2025

Today i was really just trying to integrate the meshy api into the pipeline, so you can generate abstract models. It works for the most part, but i am running into trouble. The models generated are low quality. I want them to be meshy v5 quality. So i joined the discord to ask about my problem. Will be back soon.


Aug 17, 2025

So today meshy is still on hold. But i added import export capabilities. I havent tested export capabilities yet though. I will do so tomorrow before starting. Then i will work (probably for a while i suspect, because this is getting into agentic modes i believe) but work on adding edit mode functionality.

August 18, 2025.

Apparently, the tool didnt have the foundation to be easily built on in the future , with other models, workflows, etc. so i am going through this 10 step process to make it scalable easily:
1) Config & Secrets (no more hard-coded anything)
One config file the addon and voice script can both read. Use pure-stdlib JSON/TOML so it works inside Blender.


Per-provider blocks so you can drop in more API keys and switches without editing code.


Profiles: "dev" | "prod" | "mock" to flip behavior fast.
2) Providers: a tiny plugin interface
Abstract every external model behind the same interface. Adding Meshy, ‚ÄúAnother3D,‚Äù OpenAI, Stability, local server‚Ä¶ becomes a one-file drop.
3) Commands: a single schema, many verbs
Keep natural language on the voice side; on the Blender side normalize into a small set of safe, typed commands. That makes expansion easy and safety enforceable.
4) Pipeline: one orchestrator to rule them all
Make a small state machine so every request follows the same life cycle: PARSE ‚Üí VALIDATE ‚Üí DISPATCH(provider) ‚Üí POLL ‚Üí FETCH ‚Üí POSTPROCESS ‚Üí IMPORT.
5) Process boundaries that won‚Äôt fight you
Addon (inside Blender): thin UI, command executor, importers, safety guard.


Worker (outside Blender): voice I/O, LLM parsing, provider calls, long polls.


Talk over XML-RPC or HTTP. You already have XML-RPC; that‚Äôs fine. If you ever swap to HTTP, use stdlib http.server to avoid packaging headaches.
6) Diagnostics you‚Äôll actually use
Unified logger (stdlib logging) that prefixes each line with PID, component, and a short request id. Log to file and console.


Progress callbacks: feed provider progress back to the UI panel. You already wanted this ‚Äî wire on_progress to a simple shared file or socket that the addon reads on a timer to update a progress bar.


Human-readable failures: wrap provider errors so Blender‚Äôs console shows: Provider=meshy ‚Ä¢ op=text2mesh ‚Ä¢ 404 NoMatchingRoute ‚Ä¢ tip=check endpoint /v1/text-to-3d.


7) Tests & mocks that unblock you when APIs flake
Tiny unit tests for: command parsing, safety gate, provider adapters (mock HTTP with canned JSON), and importer paths.


A mock provider that pretends to run and spits out a known .fbx from tests/fixtures/. Flip profile="mock" to demo end-to-end without internet.



August 24th, 2025

Been working on it for the past few days. adding the foundational architecture has been a task to say the least. but right now, i am working on transferring to Cursor to do the coding work, instead of going back and forth between GPT and the code.

Still have to do:

8) Safety rail that scales with capability
Your earlier ‚Äúonly add cube‚Äù restriction becomes a policy not a hack.
9) Version the public contract (future-you will thank present-you)
Add "api_version": "0.2" to every message between processes.


Reject mismatches loudly. This keeps refactors from silently breaking voice control.


10) Adding a new model tomorrow = 1 file + 3 lines
Create stb_core/providers/another3d.py implementing the 4 methods.


Register it in registry.py.


Add a block in config.json and point "active_provider": "another3d" (or select per-task).

Its october 31st, 2025. And i havent touched this in a minute, i have a lot going on right now, but i will be putting time into this every friday. 
Where i remember, its a speech to blender addon, that allows for a few functions to use your local computers microphone, openais speech to text software to transcribe, then send that transcription (i believe wrapped in a standard llm script) to gpt formatted in json with an api, then it returns a bpy script with your request, then runs the script automatically within blender. The bridge between blender and everything else, is an RPC bridge. RPC = Remote Procedure Call. You expose a few functions (e.g., add_cube, run_op, screenshot) on a tiny server. Another program calls those functions like normal code‚Äîeven though they‚Äôre running inside Blender.

## The Vision: AI-Powered 3D Creation Assistant

**Core Goal**: Create a method of integrating AI models into Blender and other 3D software that can interact with your scene in any way possible. Blender serves as the perfect benchmark because:
- I know how to use it well
- It covers a wide range of 3D capabilities and workflows (modeling, sculpting, animation, rigging, shaders, geometry nodes, etc.)
- It's open-source and extensible
- The Python API provides deep access to all functionality

**User Experience Vision**: 
For newcomers, they can jump into Blender and have an "assistant" next to them. The first thing it does is ask them what they'd like to do. The system adapts based on user experience:
- **Experienced users**: Can choose a hands-off approach where the AI works autonomously
- **Newcomers**: The AI guides them step-by-step, teaching as it goes, helping them learn while accomplishing their goal

### Four Interaction Modes

**1. Simple Assistant (Current MVP - Speech To Blender)**
- **Level**: Lowest level, call and response
- **Functionality**: 
  - Voice commands ‚Üí immediate execution (ask for something, get it returned)
  - One-shot prompting (prompt for a thing, get it created)
  - Fast, direct control over Blender operations
- **Status**: ‚úÖ Implemented and working
- **Use Case**: Quick operations, rapid iteration, hands-on control

**2. Teacher Mode**
- **Level**: Interactive learning assistant
- **Functionality**:
  - Works WITH you to accomplish projects, rather than doing it solely for you
  - Middle ground between standard Q&A LLM and full agentic workflow
  - Explains what it's doing and why
  - Suggests next steps and best practices
  - Adapts teaching style based on your experience level
- **Status**: üîÑ Planned (future phase)
- **Use Case**: Learning Blender, understanding 3D workflows, guided project completion

**3. Agent Mode**
- **Level**: Autonomous project completion
- **Functionality**:
  - Complete projects start to finish autonomously
  - Build entire scenes, models, sculpts, animations
  - Capabilities include:
    - Complete geometry node systems
    - Rigging and character animations
    - Realistic shader creation
    - Complex multi-object scenes
    - Full production pipelines
- **Status**: üîÑ Planned (future phase, builds on Super Mode)
- **Use Case**: Rapid prototyping, production work, complex multi-step projects

**4. Partnership Mode (The Big Vision)**
- **Level**: Collaborative multi-agent system
- **Functionality**:
  - AI works in the background with you, building scenes autonomously
  - Hits you up when it has questions or needs decisions
  - Can pause and wait for your input on creative choices
  - **Multi-Agent Collaboration**: Multiple specialized agents working together
    - One high-level orchestrator agent (like Grok 4 Super's approach)
    - Multiple lower-level specialized agents (modeling agent, texturing agent, animation agent, etc.)
    - Agents coordinate to build entire productions from a single prompt
- **Technical Considerations**:
  - **NVIDIA Omniverse**: Could enable true collaborative editing (multiple entities editing same file simultaneously, like Google Docs)
  - **Question**: Do we need Omniverse, or can we achieve this with Blender's Python API and smart coordination?
  - **Alternative**: File locking + intelligent task distribution + conflict resolution
- **Status**: üîÆ Long-term vision
- **Use Case**: Large-scale productions, collaborative workflows, team of AI assistants

**The Partnership Mode Vision in Detail**:
Imagine: "Create a sci-fi city scene with flying cars, neon signs, and a cyberpunk aesthetic"
- **Orchestrator Agent**: Breaks down the task, assigns work to specialists
- **Modeling Agent**: Creates buildings, vehicles, props
- **Texturing Agent**: Applies materials, creates shaders, sets up lighting
- **Animation Agent**: Adds movement, flying cars, animated signs
- **Scene Assembly Agent**: Composes everything, camera work, final polish
- All working in parallel, coordinating through the RPC bridge, asking you questions when needed
- You can jump in at any time, make adjustments, and the agents adapt

This is inspired by Grok 4 Super's approach: 1 high-level agent guiding 4 lower-level agents for a single complex task (like creating a movie). But applied to 3D creation, with the ability to talk live to agents, give feedback, and have them work in the background.

**Current Implementation Status**:
- ‚úÖ Simple Assistant (Speech To Blender) - Working
- üîÑ Super Mode Phase 1 - Infrastructure in place, enables future modes
- üîÆ Teacher/Agent/Partnership modes - Architecture being built, will leverage Super Mode foundation

**How This Vision Guides Development**:
The current work on Super Mode (Phase 1-5) is building the foundation for Agent Mode. The intelligent operation selection, mesh context analysis, and enhanced GPT prompts will enable the AI to autonomously complete complex projects.

Teacher Mode will build on Agent Mode by adding:
- Explanation generation (why each operation is chosen)
- Step-by-step guidance
- Learning-focused prompts
- User experience level detection

Partnership Mode will require:
- Multi-agent coordination system
- Task distribution and conflict resolution
- Background processing capabilities
- Real-time collaboration protocols (possibly via Omniverse or custom solution)

The RPC bridge architecture is already designed to support multiple agents - each agent can connect to the same Blender instance and coordinate through the safety gate and task queue system.

**Key Insight**: The vision keeps evolving as we build. What started as "voice control for Blender" is becoming "AI-powered 3D creation assistant with multiple interaction modes." The architecture needs to be flexible enough to support all these modes while remaining simple and maintainable.

Ok so this is where I am, and now is to get back on track, flesh out the high level idea. And focus on a low level objective. 


Ok so I think after consulting the council (gpt, gemini and claude), the MVP for this is Speech To Blender. I think I should get it working again, smoothly, video it. Then figure out next steps.



When you get in it, stay consistent, the motivation will come.




Thoughts: so I think a problem I may run into, is getting correct context for the task at hand. It may have to keep track of all the changes, and predict what the scene is going to look like, to help it suit the next request better. So almost keeping a log of all the changes as they're made, then assessing the scene and etc, and developing context of what's being made. And possible next steps


November 7, 2025 - 2:15pm

Today we focused on getting the voice command functionality working smoothly and improving performance. Major changes:

1. Fixed voice command parsing issues:
   - Added word-to-number conversion (e.g., "three" ‚Üí 3) to handle spoken numbers
   - Improved natural language matching for delete commands ("delete all" now works, not just "delete selected")
   - Added multi-command parsing - commands can now be split on "and", "then", commas, etc. (e.g., "select all and delete" executes both commands)
   - Fixed plural matching for object types ("spheres" now matches "sphere")
   - Fixed deselect command priority (checks deselect before select to avoid false matches)

2. Enabled GPT fallback for natural language understanding:
   - Set ENABLE_GPT_FALLBACK = True
   - GPT-4o now handles commands when local rules don't match
   - Works with improved local rules for better coverage

3. Added OpenAI API key management to Blender preferences:
   - Created split fields (Part 1 and Part 2) to work around Blender's ~127-character StringProperty limit
   - Full 164-character project keys (sk-proj-*) can now be stored by splitting into two fields
   - Automatic combination of parts when retrieving the key
   - UI shows combined key length and validation feedback
   - Added RPC method to retrieve key from preferences

4. Bundled openai package and added auto-install:
   - Created requirements.txt documenting dependencies
   - Added auto-install function that checks and installs openai if missing
   - Package is pre-installed in stb_runtime for distribution
   - Created BUNDLING.md documentation

5. Performance optimizations:
   - Removed unnecessary key validation test (saved ~500ms-2s per GPT call)
   - Added API key caching to avoid repeated RPC calls (saved ~20-100ms per call)
   - Reduced sleep delays: 0.2s ‚Üí 0.05s, 0.05s ‚Üí 0.01s, 0.3s ‚Üí 0.1s (saved ~150-250ms per command)
   - Disabled verbose debug mode (VERBOSE_DEBUG = False) for cleaner, faster output
   - Total performance improvement: ~670ms-2.35s faster per command

The voice workflow is now much more responsive and handles natural language commands better. The split API key fields solve the Blender storage limitation, and caching eliminates redundant RPC calls.

November 7, 2025 - 3:45pm

Today we implemented Phase 1 of "Super Mode" - an advanced AI-driven modeling mode for context-aware, intelligent object manipulation. This is the foundation for future phases that will enable complex edit mode operations, mesh analysis, and AI-guided modeling workflows.

Phase 1 Implementation (Super Mode Toggle & Basic Infrastructure):
1. Added Super Mode UI and preferences:
   - Created new "Voice Mode" panel (STB_PT_VoiceMode) in Blender's add-on UI
   - Added toggle button for "Super Mode" with visual icon feedback
   - Added "Target Object" preference field to specify which object Super Mode should focus on
   - Super Mode state stored in WindowManager property (stb_super_mode) for runtime control

2. RPC integration for Super Mode:
   - Added get_super_mode_state() RPC method to query Super Mode status from voice script
   - Returns enabled status and target_object name
   - Voice script can now check Super Mode state and route commands accordingly

3. Voice script integration:
   - Added SUPER_MODE_ENABLED and SUPER_MODE_TARGET globals in voice_to_blender.py
   - Voice script queries Super Mode state from RPC at startup and during command processing
   - Prepared infrastructure for Phase 2+ where Super Mode will route to enhanced GPT prompts with mesh context

4. Fixed console window and script stability:
   - Removed stdout/stderr redirection from subprocess.Popen to allow console window to appear
   - Created batch file wrapper (voice_to_blender.bat) for Windows to keep console open on errors
   - Added comprehensive startup checks for numpy, sounddevice, and RPC connection
   - Added 10-second pause on fatal errors to prevent silent crashes
   - Fixed syntax errors related to global variable declarations

5. Undo grouping improvements:
   - Each voice command (even multi-step ones) now creates a single undo point
   - Added start_voice_command() RPC method to mark the beginning of a voice command
   - Voice script calls this at the start of each command processing loop
   - Blender's Ctrl+Z now undoes one voice command at a time instead of all actions

Super Mode is planned as a 5-phase implementation:
- Phase 1: ‚úÖ Toggle & Basic Infrastructure (COMPLETE)
- Phase 2: Context Functions (get_modeling_context, analyze_current_mesh)
- Phase 3: Enhanced GPT Prompts with mesh analysis and reference knowledge
- Phase 4: Complex Operation Executor - Support ANY Blender operation (modifiers, transforms, topology, etc.) with GPT choosing the best approach
- Phase 5: Edit Mode Commands (loop cuts, subdivide, select by region)

The foundation is now in place for context-aware AI modeling that can understand mesh geometry, vertex positions, and choose the best Blender operations to achieve desired effects.

---

## Super Mode: Architecture & Design Philosophy

### The Core Problem

Traditional voice-to-Blender workflows are limited to simple, pre-defined commands ("add cube", "delete selected"). They can't handle complex, context-dependent modeling tasks like:
- "Make this look more like an Amazon Echo Dot" (requires understanding mesh topology, vertex patterns, and reference knowledge)
- "Add edge loops around the middle section" (requires understanding current geometry and choosing the right Blender operation)
- "Smooth out the corners but keep the overall shape" (requires mesh analysis and multi-step operations)

Super Mode addresses this by combining:
1. **Real-time mesh context** - Understanding the current state of the 3D scene
2. **Reference knowledge** - Leveraging training data from models that understand common 3D object structures
3. **Intelligent operation selection** - GPT choosing the best Blender operations to achieve desired effects
4. **Multi-step reasoning** - Breaking complex requests into sequential operations

### Models & Datasets

**Speech-to-Text: faster-whisper (small model)**
- **Why faster-whisper**: Faster inference than OpenAI's Whisper API, runs locally (privacy), no API costs for STT
- **Why "small" model**: Balance between accuracy (~95% for clear speech) and speed (~200-500ms transcription). "tiny" is too inaccurate, "base/medium" adds latency without significant gains for English
- **Language**: English-only for now (WHISPER_LANG="en") - can expand later
- **Dataset**: Trained on Common Voice, LibriSpeech, and other public speech datasets
- **Trade-off**: Local STT means no cloud dependency, but requires bundling the model (~500MB for small)

**Natural Language Understanding: GPT-4o**
- **Why GPT-4o**: 
  - Superior reasoning for complex, multi-step commands
  - Excellent at understanding context and intent
  - Strong JSON generation (critical for our structured command format)
  - Multimodal capabilities (future: can analyze viewport screenshots)
  - Fast response time (~1-3s) compared to GPT-4 Turbo
- **Why not GPT-3.5**: Less reliable for complex commands, weaker reasoning
- **Why not Claude/Sonnet**: GPT-4o has better tool-use JSON formatting, more consistent output
- **Why not local models (Ollama/Llama)**: Current local models lack the reasoning depth needed for complex 3D operations. Future: could use local model for simple commands, GPT-4o for Super Mode
- **Temperature**: 0 (deterministic) - we need consistent, reliable command generation
- **Dataset**: Trained on internet-scale data including code, technical documentation, and 3D modeling concepts

**Future: Mesh Analysis & Reference Knowledge**
- **Phase 2-3 will add**:
  - Mesh geometry extraction (vertex positions, edge topology, face data)
  - Shape analysis (detecting common patterns: cylinders, spheres, complex organic shapes)
  - Reference knowledge base (common vertex patterns for objects like Echo Dot, phone, etc.)
- **Why this matters**: GPT-4o has general knowledge about 3D objects, but doesn't know YOUR specific mesh. We need to feed it:
  1. Current mesh state (vertices, edges, faces)
  2. Target object context ("this should be an Echo Dot")
  3. Desired transformation ("make it smoother")
- **Training data considerations**: GPT-4o was trained on text/code, not raw mesh data. We bridge this by:
  - Converting mesh to structured text descriptions
  - Using GPT's understanding of geometric concepts
  - Providing operation examples in prompts

### Architecture Decisions

**Hybrid Local + Cloud Approach**
- **Local**: STT (faster-whisper), simple command parsing (regex rules), safety gate
- **Cloud**: Complex NLU (GPT-4o), future mesh analysis (could be local with specialized model)
- **Why**: Best of both worlds - fast, private for simple commands; powerful AI for complex tasks

**Two-Tier Command Processing**
1. **Normal Mode**: Fast path - local rules ‚Üí GPT fallback (simple commands)
2. **Super Mode**: Enhanced path - local rules ‚Üí GPT with mesh context ‚Üí complex operations
- **Why separate modes**: Super Mode adds latency (mesh analysis, richer prompts). Users should opt-in for complex tasks.

**RPC Bridge Architecture**
- **Why XML-RPC**: Simple, stdlib-only, works inside Blender's Python environment
- **Why not HTTP/REST**: More complex, requires additional dependencies
- **Why not WebSocket**: Overkill for request/response pattern, adds complexity
- **Safety Gate**: Whitelist approach - only safe operators allowed. Prevents dangerous operations (file deletion, addon disabling, etc.)

**Undo Grouping Strategy**
- **Problem**: Multiple Blender operations from one voice command should be one undo step
- **Solution**: `bpy.ops.ed.undo_push(message="Voice Command")` after each command batch
- **Why**: User expects "Ctrl+Z" to undo their voice command, not individual operations

### Why Not Alternative Approaches?

**Why not pure local models?**
- Current local LLMs (Llama 3, Mistral) lack the reasoning depth for complex 3D operations
- Future: Could use local model for simple commands, GPT-4o for Super Mode (hybrid)

**Why not specialized 3D AI models?**
- No general-purpose 3D modeling AI exists yet (Shape-E, Point-E are generation-only)
- GPT-4o's general reasoning + our mesh context bridge is more flexible

**Why not rule-based system?**
- Rules work for simple commands ("add cube") but don't scale to complex requests
- GPT provides flexibility and natural language understanding

**Why not agentic workflow (autonomous agent)?**
- Too risky for Blender operations (could make unintended changes)
- User should maintain control - voice commands are explicit instructions, not autonomous agents

### Future Considerations

**Phase 2-5 will explore**:
- Mesh geometry extraction and analysis
- Reference knowledge integration (common object patterns)
- Edit mode operations (loop cuts, subdivide, vertex manipulation)
- Modifier-based operations (subdivision surface, bevel, etc.)
- Multi-object operations (coordinated changes across scene)

**Potential improvements**:
- Local model fallback for simple Super Mode commands (reduce API costs)
- Specialized mesh analysis model (trained on 3D geometry)
- Viewport screenshot analysis (GPT-4o vision for visual feedback)
- Learning from user corrections (fine-tuning on successful operations)

The goal is to make Super Mode feel like working with an expert 3D artist who understands both your intent and the technical details of Blender operations.

---

## Intelligent Operation Selection: How It Works

### Multi-Tier Selection Pipeline

The system uses a cascading approach to select the best Blender operation for each voice command:

**Tier 1: I/O Rules (Fastest - ~1ms)**
- Handles file import/export commands
- Pattern matching: "import [filepath]", "export [filepath]"
- Direct mapping to Blender import operators (STL, PLY, FBX, OBJ, GLTF, USD)
- Auto-enables required Blender add-ons if missing (e.g., `io_mesh_stl` for STL imports)

**Tier 2: Local Rules (Fast - ~5-10ms)**
- Regex-based pattern matching for common commands
- Handles ~50+ common operations without API calls
- Examples:
  - "add cube" ‚Üí `mesh.primitive_cube_add`
  - "delete selected" ‚Üí `object.delete`
  - "select all" ‚Üí `object.select_all(action="SELECT")`
  - "move by 1 2 3" ‚Üí `transform.translate(value=(1,2,3))`
  - "scale up" ‚Üí `transform.resize(value=(1.2,1.2,1.2))`
- Supports word numbers ("add three cubes" ‚Üí quantity=3)
- Handles plural forms ("spheres" matches "sphere")
- Multi-command parsing ("add cube and add sphere" ‚Üí two operations)

**Tier 3: GPT-4o Fallback (Slower - ~1-3s)**
- Used when local rules don't match
- GPT-4o analyzes natural language and selects appropriate Blender operations
- Returns structured JSON: `{"op": "module.operator", "kwargs": {...}}`
- Can return multiple operations as a JSON array for complex commands

### How GPT-4o Selects Operations

**System Prompt (Current Implementation)**
```
You are a Blender automation agent.
Output ONLY raw JSON (no prose, no code fences).
Each command must be of the form: {"op":"<module.op>","kwargs":{}}.
If multiple steps are implied, output a JSON array of such dicts.
Prefer creative operators (object/mesh/curve/transform/material/node/render).
Never use file/quit/addon/script/image.save operators.
```

**User Prompt**
```
Instruction: [voice command transcript]
Return JSON only.
```

**GPT's Decision Process:**
1. **Intent Recognition**: Understands what the user wants to achieve
   - "Make this smoother" ‚Üí subdivision surface or bevel operations
   - "Add detail to the edges" ‚Üí edge loops or bevel
   - "Create a rounded corner" ‚Üí bevel or proportional editing

2. **Operation Mapping**: Maps intent to Blender operators
   - Uses GPT-4o's training knowledge of Blender's operator system
   - Understands operator naming conventions (`mesh.primitive_cube_add`, `object.modifier_add`)
   - Knows which operators are appropriate for different contexts

3. **Parameter Extraction**: Extracts values from natural language
   - "Move 2 meters up" ‚Üí `transform.translate(value=(0,0,2))`
   - "Rotate 45 degrees on X" ‚Üí `transform.rotate(value=(0.785,0,0))` (radians)
   - "Scale to 1.5" ‚Üí `transform.resize(value=(1.5,1.5,1.5))`

4. **Multi-Step Reasoning**: Breaks complex commands into sequences
   - "Add a cube and move it up" ‚Üí `[{"op":"mesh.primitive_cube_add"}, {"op":"transform.translate","kwargs":{"value":(0,0,1)}}]`
   - "Create a sphere, scale it, and add a material" ‚Üí array of 3 operations

5. **Context Awareness** (Future - Phase 2-3):
   - Will analyze current mesh state (vertex count, topology, modifiers)
   - Will consider target object context ("this should be an Echo Dot")
   - Will choose operations based on current Blender mode (Object/Edit/Sculpt)

**Example GPT Selections:**
- "Add a torus with major radius 2" ‚Üí `{"op":"mesh.primitive_torus_add","kwargs":{"major_radius":2}}`
- "Subdivide the selected mesh" ‚Üí `{"op":"mesh.subdivide","kwargs":{}}`
- "Add a bevel modifier" ‚Üí `{"op":"object.modifier_add","kwargs":{"type":"BEVEL"}}`
- "Delete all objects" ‚Üí `{"op":"object.select_all","kwargs":{"action":"SELECT"}}, {"op":"object.delete"}`

### Safety Gate Validation

Before any operation executes, it passes through a safety gate:

**Blacklist (Dangerous Operators - Always Blocked)**
- `file.quit` - Quitting Blender
- `wm.quit_blender` - Quitting Blender
- `script.reload` - Reloading scripts
- `preferences.addon_disable` - Disabling add-ons
- `preferences.addon_remove` - Removing add-ons

**Whitelist (Safe Operator Prefixes - Always Allowed)**
- `mesh.*` - Mesh operations (add, edit, modify)
- `object.*` - Object operations (select, delete, duplicate, transform)
- `transform.*` - Transform operations (translate, rotate, scale)
- `view3d.*` - Viewport operations (view_selected, frame_selected)
- `import_scene.*` - Import operations (FBX, OBJ, GLTF, USD)
- `wm.obj_import`, `wm.gltf_import`, `wm.usd_import` - Modern import operators

**Default Behavior**
- Operators not in blacklist are allowed (with warning)
- This allows GPT to use creative operators (curves, materials, nodes) while blocking dangerous ones

**File Path Validation**
- Import operators validate file paths exist before execution
- Prevents errors from invalid file paths

### Operation Execution Flow

1. **Voice Command Received** ‚Üí Transcribed by faster-whisper
2. **Command Parsing**:
   - Try I/O rules ‚Üí if match, execute and return
   - Try local rules ‚Üí if match, execute and return
   - Try GPT-4o ‚Üí if match, execute and return
3. **Safety Gate Check** ‚Üí Validate operator is safe
4. **Add-on Auto-Enable** ‚Üí Enable required import add-ons if needed
5. **Queue Operation** ‚Üí Add to `_TASKQ` (thread-safe queue)
6. **Main Thread Execution** ‚Üí Blender's timer (`bpy.app.timers`) drains queue
7. **Undo Push** ‚Üí After command batch completes, push undo point
8. **Result** ‚Üí Success/failure message returned to voice script

### Super Mode Enhancements (Future Phases)

**Phase 2: Context Functions**
- `get_modeling_context()`: Returns current scene state (selected objects, active mode, modifiers)
- `analyze_current_mesh()`: Extracts mesh geometry (vertex count, edge loops, face topology)
- GPT will receive this context in the prompt, enabling smarter operation selection

**Phase 3: Enhanced GPT Prompts**
- System prompt will include:
  - Current mesh analysis (vertex positions, topology patterns)
  - Target object context ("building an Echo Dot")
  - Available Blender operations for the current mode
- GPT will choose operations based on:
  - Current geometry state
  - Desired outcome
  - Best practices for achieving the effect

**Phase 4: Complex Operation Executor**
- GPT will select from ANY Blender operation (not just whitelisted prefixes)
- Will choose the best approach for complex tasks:
  - Modifiers vs direct mesh editing
  - Loop cuts vs edge loops vs subdivision
  - Transform operations vs proportional editing
- Will chain multiple operations intelligently

**Phase 5: Edit Mode Commands**
- GPT will understand edit mode context
- Will select appropriate edit mode operations:
  - `mesh.loopcut` for adding edge loops
  - `mesh.subdivide` for adding geometry
  - `mesh.select_all` with different actions for vertex/edge/face selection
- Will maintain selection state across operations

### Why This Approach Works

**Speed**: Local rules handle 80%+ of common commands instantly (no API latency)

**Flexibility**: GPT handles complex, natural language commands that rules can't cover

**Safety**: Safety gate prevents dangerous operations while allowing creative ones

**Scalability**: Adding new operations is easy:
- Simple commands ‚Üí add to local rules
- Complex commands ‚Üí GPT handles automatically
- Dangerous operations ‚Üí add to blacklist

**User Experience**: 
- Fast response for common commands
- Natural language understanding for complex requests
- Consistent behavior (deterministic with temperature=0)

### Current Limitations & Future Improvements

**Limitations:**
- GPT doesn't know current mesh state (Phase 2 will fix)
- GPT doesn't know Blender mode (Object/Edit/Sculpt) - Phase 5 will fix
- GPT may choose suboptimal operations for complex tasks - Phase 3-4 will improve
- No learning from user corrections - future improvement

**Improvements Coming:**
- Mesh context in GPT prompts (Phase 2-3)
- Mode-aware operation selection (Phase 5)
- Operation quality scoring (future)
- User feedback loop for operation selection (future)

The intelligent operation selection system balances speed, safety, and flexibility to provide a natural voice interface to Blender while maintaining control and preventing dangerous operations.

---

December 19, 2025

Today's Accomplishments:

1. Voice Listening Toggle (Alt+F):
   - Added Alt+F keyboard shortcut to toggle voice listening on/off while RPC server is running
   - Voice listening starts automatically when RPC server starts
   - Added RPC methods: get_voice_listening_state() and set_voice_listening_state()
   - Voice script checks listening state before recording, after recording, and before sending commands
   - Added UI status indicator in RPC Bridge panel showing listening state (ON/OFF)
   - Improved toggle responsiveness: reduced check interval from 0.5s to 0.1s when listening is disabled
   - Added UI redraw on toggle for immediate visual feedback
   - Commands are now properly blocked when listening is disabled, even if recording was in progress

2. Improved Silence Detection:
   - Switched from cumulative silence time to consecutive silent blocks approach (more reliable)
   - Requires continuous silence blocks instead of total silence time
   - If speech interrupts silence, the counter resets (prevents false stops)
   - Added MAX_RECORD_SEC = 10.0s safety timeout to prevent infinite recording loops
   - Added debug output (when VERBOSE_DEBUG enabled) showing silence detection state
   - Current settings: SILENCE_RMS=300, SILENCE_HOLD=0.6s, MIN_SPOKEN=0.3s

3. Bug Fixes:
   - Fixed analyze_current_mesh() context error: Changed bpy.context.active_object to context.view_layer.objects.active (correct Blender 4.x API)
   - Fixed voice script continuing to process commands after listening was toggled off
   - Improved state checking to prevent commands from being sent when listening is disabled

4. Code Quality:
   - Better error handling for listening state checks
   - Added safety timeouts to prevent infinite loops
   - Improved debug output for diagnosing silence detection issues

Current Status of 5-Phase Super Mode Plan:

- Phase 1: ‚úÖ COMPLETE - Super Mode toggle & basic infrastructure
- Phase 2: ‚úÖ COMPLETE - Context functions (get_modeling_context, analyze_current_mesh)
- Phase 3: ‚úÖ COMPLETE - Enhanced GPT prompts with mesh analysis and reference knowledge
- Phase 4: üîÑ IN PROGRESS - Complex operation executor (support ANY Blender operation)
  - Current issue: Some material/node operations failing (material.new, node.add_node, etc.)
  - Need to fix: Material creation, node editor operations, proper context switching
  - Goal: GPT chooses best approach for any Blender operation (modifiers, transforms, topology, materials, nodes, etc.)
- Phase 5: ‚è≥ NOT STARTED - Edit Mode Commands (loop cuts, subdivide, select by region)

Next Steps:

1. Fix Phase 4 Material/Node Operations:
   - Fix material.new operator (incorrect kwargs - should use bpy.data.materials.new in Python code execution)
   - Fix node editor operations (require proper context - need to switch to Shader Editor or use Python code execution)
   - Ensure all Blender operations work correctly through the execute RPC method
   - Test complex material creation workflows
   - Test node editor operations (shader nodes, geometry nodes)

2. Complete Phase 4:
   - Ensure GPT can choose from ANY Blender operation
   - Test complex multi-step operations (modifiers, transforms, topology changes)
   - Verify operation chaining works correctly
   - Test edge cases and error handling

3. Begin Phase 5 (Edit Mode Commands):
   - Implement edit mode context detection
   - Add edit mode operation support (mesh.loopcut, mesh.subdivide, etc.)
   - Handle selection state management in edit mode
   - Test edit mode workflows

4. Continue Refinement:
   - Monitor silence detection performance and adjust if needed
   - Test voice listening toggle in various scenarios
   - Gather user feedback on responsiveness and accuracy
   - Optimize performance where possible

5. Future Enhancements (Post-Phase 5):
   - Teacher Mode implementation
   - Agent Mode implementation
   - Partnership Mode architecture
   - Learning from user corrections
   - Operation quality scoring

---

## January 9, 2025 - Quality Assessment & Target Matching

### Major Accomplishments:

1. **Enhanced Quality Assessment with Target Matching**:
   - Problem: AI was creating basic primitives without checking if they actually looked like the target object (e.g., iPhone)
   - Solution: Enhanced `assess_object_quality()` RPC method with target-specific checks:
     - iPhone: requires body, screen, camera; checks for buttons, speaker, port
     - Car: requires body, 4 wheels; checks for windows, lights, spoiler
     - Cup/Mug: requires body, handle
   - Calculates `target_match_score` (0.0-1.0) based on required parts found
   - Combines technical quality (40%) + target match (60%) for overall quality score

2. **Mandatory Quality Check Before Finishing**:
   - Problem: AI could finish with incomplete objects that didn't resemble the target
   - Solution: Added automatic quality assessment when AI tries to finish
   - Requires `quality_score >= 0.5` AND `target_match_score >= 0.5`
   - Forces AI to continue refining if quality is too low
   - Shows missing features and actionable guidance
   - Prevents premature completion with incomplete objects

3. **Enhanced System Prompt**:
   - Added "MANDATORY QUALITY CHECK BEFORE FINISHING" section
   - Instructs AI to check quality before finishing
   - Emphasizes object must match target before completion
   - Added comprehensive guide to advanced Blender operations (edit mode, modifiers)

4. **Improved Assessment Display**:
   - Shows both overall quality and target match scores separately
   - Lists found features and missing features
   - Provides actionable guidance based on scores
   - Better formatted output for AI decision-making

### Current Status:

- ‚úÖ Target-aware quality assessment implemented
- ‚úÖ Mandatory quality checks before finishing
- ‚úÖ Enhanced system prompt with quality requirements
- ‚úÖ Support for iPhone, Car, and Cup/Mug target checks
- ‚ö†Ô∏è **NOT YET TESTED** - Quality checks need testing with real object creation

### Next Steps:

1. **Test Quality Checks**:
   - Test with "Create an iPhone 16" command
   - Verify quality check blocks finishing when parts are missing
   - Verify quality check allows finishing when all parts are present
   - Test with other objects (car, cup, etc.)

2. **Expand Target Support**:
   - Add more object types (furniture, electronics, vehicles, etc.)
   - Improve part detection logic
   - Add proportion/scale checks

3. **Refine Quality Scoring**:
   - Tune quality score thresholds
   - Add visual similarity checks (if possible)
   - Consider screenshot-based validation

4. **Performance Optimization**:
   - Cache assessment results if scene hasn't changed
   - Optimize part detection algorithm

### Technical Notes:

- Quality assessment runs automatically before finish action
- Target match score is calculated based on required parts found in object names
- Assessment can be manually triggered with `observe assess_object_quality`
- Quality checks only run when `target_object` is provided (backward compatible)

---

## December 19, 2025 - 11:58AM

### Major Accomplishments:

1. **Fixed XML-RPC Serialization Error**:
   - Problem: `analyze_current_mesh()` was returning dictionaries with integer keys (face_types), which XML-RPC cannot serialize
   - Solution: Converted all dictionary keys to strings in `analyze_current_mesh()` (face_types now uses string keys)
   - Updated `voice_to_blender.py` to handle string keys when reading face_types data
   - This fixes the `TypeError: dictionary key must be string` errors that were crashing context fetching

2. **Fixed Duplicate Operation Detection**:
   - Problem: Operations like `object.select_all` and `object.join` were being incorrectly skipped as duplicates, preventing valid state-changing operations
   - Solution: Added `allow_repeat_ops` set that allows state-changing operations to repeat:
     - `object.select_all`, `object.join`, `object.delete`, `mesh.select_all`, `object.mode_set`, etc.
   - These operations can now run multiple times in a single ReAct loop when needed

3. **Enhanced Error Handling**:
   - Added try/except blocks around `rpc.analyze_current_mesh()` calls in `_react_observe()`
   - Better error messages for serialization issues and missing mesh objects
   - Prevents observation loops from crashing on XML-RPC errors

4. **Screenshot Capture Improvements**:
   - Screenshot capture now works consistently using PIL.ImageGrab and mss libraries
   - Always fetches fresh screenshots for each command (not cached)
   - Better error handling and fallback methods

5. **Stateful Workflow**:
   - Conversation history is now being saved and loaded correctly
   - History includes images (screenshots) when available
   - Context caching for modeling/mesh/scene data (but fresh screenshots each time)

### Current Status:

- ‚úÖ Screenshot capture working consistently
- ‚úÖ Conversation history tracking working
- ‚úÖ Image integration into GPT calls working
- ‚úÖ XML-RPC serialization errors fixed
- ‚úÖ Duplicate operation detection improved
- ‚úÖ ReAct framework executing commands successfully (iPhone 16 creation worked well)
- ‚ö†Ô∏è Some observation loops still occurring (but detection and intervention working)
- ‚ö†Ô∏è Some operations still getting stuck (e.g., `object.join` in shoe creation)

### Next Steps:

1. **Investigate ReAct Loop Issues**:
   - Why is `object.join` getting stuck in loops?
   - Improve observation feedback to be more actionable
   - Consider adding more explicit guidance in system prompt about when to finish vs continue

2. **Test More Complex Scenarios**:
   - Test with various object types (shoes, mugs, phones, etc.)
   - Verify screenshot capture works in all scenarios
   - Test conversation history persistence across multiple commands

3. **Refine ReAct System Prompt**:
   - Add more explicit rules about when to use FINISH vs continue
   - Better guidance on handling failed operations
   - Clearer instructions on using observations effectively

4. **Performance Optimization**:
   - Consider caching screenshot capture if scene hasn't changed
   - Optimize context fetching to reduce RPC calls
   - Monitor API usage and costs

### Technical Notes:

- XML-RPC requires all dictionary keys to be strings (not integers)
- State-changing operations (select, join, delete) should be allowed to repeat
- Screenshot capture works best with PIL.ImageGrab or mss (faster than Blender render)
- Conversation history helps GPT maintain context across multiple commands
